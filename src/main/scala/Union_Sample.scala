import org.apache.spark.sql.SparkSession

object Union_Sample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Sample Union")
      .master("local[*]")
      .getOrCreate()
    import spark.sqlContext.implicits._
    import spark.implicits._
    val data = Seq(
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101),
      ("2021-09-15", 1),
      ("2021-09-16", 2),
      ("2021-09-17", 10),
      ("2021-09-18", 25),
      ("2021-09-19", 500),
      ("2021-09-20", 50),
      ("2021-09-21", 100),
      ("2021-09-15", 10),
      ("2021-09-16", 20),
      ("2021-09-17", 10),
      ("2021-09-18", 250),
      ("2021-09-19", 500),
      ("2021-09-20", 55),
      ("2021-09-30", 101)
    )
    var df = spark.createDataFrame(data).toDF("date", "id")

    df = df.union(df)
    df = df.filter(df("id") === 101)
    // df.explain(cost = true)
    /*
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    df = df.union(df)
    println(df.groupBy("id").count())
    */
    //println(df.count())
  }
}
